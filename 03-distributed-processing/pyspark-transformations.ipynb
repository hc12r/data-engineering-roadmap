{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PySpark Transformations Tutorial (Complete)\n",
    "\n",
    "This notebook is a complete, self-contained walkthrough of core PySpark transformations.\n",
    "It is provided as markdown-first with illustrative code examples so it renders well\n",
    "without requiring a PySpark runtime in this environment. Copy any code cell into your\n",
    "Spark-capable environment to execute it.\n",
    "\n",
    "What you will learn:\n",
    "- How to create a SparkSession\n",
    "- How to build small DataFrames from in-memory data\n",
    "- Core transformations: select, withColumn, filter, groupBy/agg\n",
    "- Joins and broadcast joins\n",
    "- Window functions (row_number, rank, lag)\n",
    "- Practical optimization tips: cache/persist, repartition/coalesce, explain plans\n",
    "- Simple I/O patterns and temp views for SQL\n"
   ],
   "id": "2e5a6ae2bfbf4d37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0) Environment setup\n",
    "\n",
    "If you are running this in a plain Python environment, install PySpark first:\n",
    "\n",
    "\n"
   ],
   "id": "a99c0f52c5268de6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install pyspark",
   "id": "d76b3358cb57b63b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Then start your SparkSession:",
   "id": "630a1b1dcde9bc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"pyspark-transformations-demo\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "id": "8087dd5df205449a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Create sample DataFrames\n",
    "\n",
    "We'll use tiny, in-memory datasets for transactions and customers.\n",
    "\n",
    "```python\n",
    "transactions = [\n",
    "    (1, '2025-11-01', 101, 120.50, 'SUCCESS'),\n",
    "    (2, '2025-11-01', 102,  75.00, 'FAILED'),\n",
    "    (3, '2025-11-02', 101,  15.99, 'SUCCESS'),\n",
    "    (4, '2025-11-02', 103, 220.00, 'PENDING'),\n",
    "    (5, '2025-11-03', 104,  50.00, 'SUCCESS'),\n",
    "    (6, '2025-11-03', 101,  80.00, 'SUCCESS'),\n",
    "    (7, '2025-11-04', 105,  20.00, 'SUCCESS'),\n",
    "]\n",
    "customers = [\n",
    "    (101, 'Ana',   'MOZ'),\n",
    "    (102, 'Carlos','MOZ'),\n",
    "    (103, 'Asha',  'TZA'),\n",
    "    (104, 'Musa',  'KEN'),\n",
    "    # 105 intentionally missing to show join behavior\n",
    "]\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "tx_df = spark.createDataFrame(transactions, ['tx_id','tx_date','customer_id','amount','status'])\n",
    "cust_df = spark.createDataFrame(customers, ['customer_id','customer_name','country'])\n",
    "\n",
    "tx_df.show()\n",
    "cust_df.show()\n",
    "```\n"
   ],
   "id": "36f1716e8d1ad963"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) Select, withColumn, filter\n",
    "\n",
    "Add derived columns and filter rows.\n",
    "\n",
    "```python\n",
    "enriched_df = (\n",
    "    tx_df\n",
    "    .select('tx_id','tx_date','customer_id','amount','status')\n",
    "    .withColumn('tx_dt', F.to_date('tx_date'))\n",
    "    .withColumn('is_success', F.col('status') == F.lit('SUCCESS'))\n",
    "    .withColumn('fee', F.round(F.col('amount') * F.lit(0.02), 2))\n",
    ")\n",
    "\n",
    "enriched_df.filter('is_success').show()\n",
    "enriched_df.printSchema()\n",
    "```\n"
   ],
   "id": "a92538940a7ac9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) GroupBy and Aggregations\n",
    "\n",
    "Compute total successful amounts per customer and per day.\n",
    "\n",
    "```python\n",
    "success_df = enriched_df.filter(F.col('is_success'))\n",
    "\n",
    "by_customer = success_df.groupBy('customer_id').agg(\n",
    "    F.count('*').alias('success_count'),\n",
    "    F.round(F.sum('amount'), 2).alias('total_amount'),\n",
    ")\n",
    "\n",
    "by_day = success_df.groupBy('tx_dt').agg(\n",
    "    F.round(F.sum('amount'), 2).alias('total_amount'),\n",
    ")\n",
    "\n",
    "by_customer.show()\n",
    "by_day.orderBy('tx_dt').show()\n",
    "```\n"
   ],
   "id": "e053e650487d7d56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Joins and Broadcast Joins\n",
    "\n",
    "Join transactions with customers. Use broadcast when the right side is small.\n",
    "\n",
    "```python\n",
    "# Regular left join\n",
    "joined = tx_df.join(cust_df, on='customer_id', how='left')\n",
    "joined.select('tx_id','customer_id','customer_name','country','amount','status').show()\n",
    "\n",
    "# Broadcast join (hint)\n",
    "bjoined = tx_df.hint('broadcast').join(cust_df.hint('broadcast'), 'customer_id', 'left')\n",
    "bjoined.explain()\n",
    "bjoined.show()\n",
    "```\n"
   ],
   "id": "ebd228725870b436"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Window Functions\n",
    "\n",
    "Compute row_number per customer and daily ranks and lags.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w_by_customer_date = Window.partitionBy('customer_id').orderBy('tx_dt')\n",
    "w_daily = Window.partitionBy('tx_dt').orderBy(F.col('amount').desc())\n",
    "\n",
    "win_df = (\n",
    "    enriched_df\n",
    "    .withColumn('rn_per_customer', F.row_number().over(w_by_customer_date))\n",
    "    .withColumn('rank_daily_amount', F.rank().over(w_daily))\n",
    "    .withColumn('prev_amount', F.lag('amount', 1).over(w_by_customer_date))\n",
    ")\n",
    "\n",
    "win_df.orderBy('customer_id','tx_dt','rn_per_customer').show()\n",
    "```\n"
   ],
   "id": "5b42aca39ac132ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6) Partitioning and Caching\n",
    "\n",
    "Repartition/coalesce and cache for repeated use.\n",
    "\n",
    "```python\n",
    "# Repartition by customer for potential join/group-by locality\n",
    "rp = success_df.repartition('customer_id')\n",
    "print('Partitions (repartitioned):', rp.rdd.getNumPartitions())\n",
    "\n",
    "# Coalesce to reduce partitions without full shuffle\n",
    "co = rp.coalesce(2)\n",
    "print('Partitions (coalesced):', co.rdd.getNumPartitions())\n",
    "\n",
    "# Cache if reused multiple times\n",
    "co_cached = co.cache()\n",
    "co_cached.count()  # materialize\n",
    "co_cached.groupBy('customer_id').agg(F.sum('amount')).show()\n",
    "```\n"
   ],
   "id": "c7d7049594a65ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7) I/O and Explain Plans\n",
    "\n",
    "Write small output to a local temp folder and display query plan.\n",
    "\n",
    "```python\n",
    "out_path = '/tmp/pyspark_transformations_demo'\n",
    "# Use overwrite mode to keep runs idempotent in local tests\n",
    "by_customer.write.mode('overwrite').parquet(out_path)\n",
    "print('Wrote output to:', out_path)\n",
    "\n",
    "# Explain a representative query\n",
    "by_customer.explain()\n",
    "\n",
    "# Register a temp view for SQL examples\n",
    "win_df.createOrReplaceTempView('tx_enriched')\n",
    "spark.sql(\"SELECT customer_id, COUNT(*) AS cnt FROM tx_enriched GROUP BY customer_id\").show()\n",
    "```\n"
   ],
   "id": "c7a1ff15e618eed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8) Best Practices Recap\n",
    "\n",
    "- Prefer DataFrames over RDDs for optimization via Catalyst.\n",
    "- Use predicate pushdown by filtering early when reading from columnar formats.\n",
    "- Broadcast small dimension tables in skewed or small-large joins.\n",
    "- Manage partitions: align with join/aggregation keys, avoid too many tiny partitions.\n",
    "- Cache only when reusing data; unpersist if no longer needed.\n",
    "- Validate plans with `explain()` and monitor shuffles and stages in the UI.\n",
    "\n",
    "See also in this folder:\n",
    "- spark-architecture.md\n",
    "- optimization-techniques.md\n"
   ],
   "id": "13e6dc7648aa1920"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9) Cleanup\n",
    "\n",
    "If running interactively, stop your session when done:\n",
    "\n",
    "```python\n",
    "spark.stop()\n",
    "print('Spark session stopped.')\n",
    "```"
   ],
   "id": "978c6e8e501c775"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
